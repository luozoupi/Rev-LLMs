"""
Quick Test Script for Qwen3 Evaluation Suite
===========================================

This script provides a quick test to verify the evaluation framework is working
and demonstrates usage with minimal examples.
"""

import sys
import os
import torch
import logging
from pathlib import Path

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_model_creation():
    """Test model creation"""
    logger.info("Testing model creation...")
    
    try:
        # Try to import the actual model creation functions
        try:
            from qwen3_reversible_02_3 import create_modern_reversible_qwen3_model
            logger.info("‚úì Found modern reversible model creation function")
            
            # Test small model creation
            model = create_modern_reversible_qwen3_model(
                vocab_size=1000,
                hidden_size=256,
                num_hidden_layers=4,
                num_attention_heads=8,
                num_key_value_heads=4,
                attention_type="standard",
                use_reversible=True,
                device="cpu"
            )
            logger.info(f"‚úì Reversible model created: {sum(p.numel() for p in model.parameters())} parameters")
            
            # Test standard model (reversible=False)
            model = create_modern_reversible_qwen3_model(
                vocab_size=1000,
                hidden_size=256,
                num_hidden_layers=4,
                num_attention_heads=8,
                num_key_value_heads=4,
                attention_type="standard",
                use_reversible=False,
                device="cpu"
            )
            logger.info(f"‚úì Standard model created: {sum(p.numel() for p in model.parameters())} parameters")
            
            return True
            
        except ImportError:
            # Fallback to run_evaluation ModelFactory
            from run_evaluation import ModelFactory
            
            # Test small model creation
            model = ModelFactory.create_model("reversible_qwen3", "small", "cpu")
            logger.info(f"‚úì Reversible model created: {sum(p.numel() for p in model.parameters())} parameters")
            
            model = ModelFactory.create_model("standard_qwen3", "small", "cpu")
            logger.info(f"‚úì Standard model created: {sum(p.numel() for p in model.parameters())} parameters")
            
            return True
        
    except Exception as e:
        logger.error(f"‚úó Model creation failed: {e}")
        return False

def test_zero_order_optimization():
    """Test zero-order optimization"""
    logger.info("Testing zero-order optimization...")
    
    try:
        from zero_order_optimization import ZeroOrderConfig, ZeroOrderOptimizer
        import torch
        
        # Try to create model
        try:
            from qwen3_reversible_02_3 import create_modern_reversible_qwen3_model
            # Create small model
            model = create_modern_reversible_qwen3_model(
                vocab_size=1000,
                hidden_size=128,
                num_hidden_layers=2,
                num_attention_heads=4,
                num_key_value_heads=2,
                attention_type="standard",
                use_reversible=True,
                device="cpu"
            )
        except ImportError:
            from run_evaluation import ModelFactory
            # Create small model
            model = ModelFactory.create_model("reversible_qwen3", "small", "cpu")
        
        # Create ZO optimizer
        zo_config = ZeroOrderConfig(zo_eps=1e-3, enhanced=True)
        optimizer = ZeroOrderOptimizer(model, zo_config, learning_rate=1e-3)
        
        # Test one step
        batch = {
            'input_ids': torch.randint(0, 1000, (2, 64)),
            'attention_mask': torch.ones((2, 64)),
            'labels': torch.randint(0, 2, (2,))
        }
        
        def simple_loss_fn(outputs, batch):
            if hasattr(outputs, 'logits'):
                logits = outputs.logits if hasattr(outputs, 'logits') else outputs['logits']
                return torch.nn.functional.cross_entropy(logits.mean(dim=1), batch['labels'])
            else:
                return torch.mean(outputs if isinstance(outputs, torch.Tensor) else outputs['logits'])
        
        step_result = optimizer.step(batch, simple_loss_fn)
        logger.info(f"‚úì ZO optimization step completed: loss={step_result['loss']:.4f}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚úó Zero-order optimization test failed: {e}")
        return False

def test_dataset_loading():
    """Test dataset loading"""
    logger.info("Testing dataset loading...")
    
    try:
        from run_evaluation import DatasetLoader
        from transformers import AutoTokenizer
        
        # Use a simple tokenizer
        tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        loader = DatasetLoader(tokenizer)
        
        # Test memory stress dataset
        memory_dataset = loader.create_memory_stress_dataset([256, 512], num_samples=20)
        logger.info(f"‚úì Memory stress dataset created: {len(memory_dataset)} samples")
        
        return True
        
    except Exception as e:
        logger.error(f"‚úó Dataset loading test failed: {e}")
        return False

def test_analysis_tools():
    """Test analysis and comparison tools"""
    logger.info("Testing analysis tools...")
    
    try:
        from analysis_and_comparison import StatisticalAnalyzer, MemoryAnalyzer
        import numpy as np
        
        # Test statistical analyzer
        stat_analyzer = StatisticalAnalyzer()
        
        # Generate sample data
        reversible_scores = np.random.normal(0.75, 0.05, 10)
        standard_scores = np.random.normal(0.73, 0.04, 10)
        
        comparison = stat_analyzer.compare_performance(reversible_scores, standard_scores, "test_metric")
        logger.info(f"‚úì Statistical comparison completed: gap={comparison['comparison']['difference']:.3f}")
        
        # Test memory analyzer
        memory_analyzer = MemoryAnalyzer()
        
        # Sample memory profiles
        sample_profiles = {
            'model1': {
                'memory_profiles': {
                    'length_256': {'memory_used_mb': 100},
                    'length_512': {'memory_used_mb': 200},
                    'length_1024': {'memory_used_mb': 400},
                }
            }
        }
        
        scaling_analysis = memory_analyzer.analyze_memory_scaling(sample_profiles)
        logger.info(f"‚úì Memory scaling analysis completed: {len(scaling_analysis)} models analyzed")
        
        return True
        
    except Exception as e:
        logger.error(f"‚úó Analysis tools test failed: {e}")
        return False

def run_quick_evaluation():
    """Run a minimal evaluation"""
    logger.info("Running quick evaluation...")
    
    try:
        from run_evaluation import EvaluationConfig, Qwen3EvaluationSuite
        
        # Create minimal config
        config = EvaluationConfig(
            model_names=["reversible_qwen3"],
            model_sizes=["small"],
            tasks=["memory_stress"],  # Use only custom task to avoid GLUE dependencies
            max_samples_per_task=10,
            num_epochs=1,
            batch_size=2,
            seeds=[42],
            device="cpu",
            mixed_precision=False,
        )
        
        # Run evaluation
        suite = Qwen3EvaluationSuite(config)
        
        # Mock the evaluation to avoid heavy computation
        logger.info("‚úì Evaluation suite initialized successfully")
        logger.info("‚úì Quick evaluation test passed (mocked)")
        
        return True
        
    except Exception as e:
        logger.error(f"‚úó Quick evaluation failed: {e}")
        return False

def main():
    """Run all tests"""
    logger.info("Starting Qwen3 Evaluation Suite Quick Test")
    logger.info("=" * 50)
    
    tests = [
        ("Model Creation", test_model_creation),
        ("Zero-Order Optimization", test_zero_order_optimization),
        ("Dataset Loading", test_dataset_loading),
        ("Analysis Tools", test_analysis_tools),
        ("Quick Evaluation", run_quick_evaluation),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nRunning test: {test_name}")
        try:
            if test_func():
                passed += 1
            else:
                logger.error(f"Test {test_name} failed")
        except Exception as e:
            logger.error(f"Test {test_name} crashed: {e}")
    
    logger.info("=" * 50)
    logger.info(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All tests passed! The evaluation suite is ready to use.")
        logger.info("\nNext steps:")
        logger.info("1. Install required dependencies: pip install -r requirements.txt")
        logger.info("2. Run quick evaluation: python run_evaluation.py --config configs/quick_eval.yaml")
        logger.info("3. Run comprehensive evaluation: python run_evaluation.py --config configs/comprehensive_eval.yaml")
    else:
        logger.info(f"‚ùå {total - passed} tests failed. Check the errors above.")
        logger.info("\nTroubleshooting:")
        logger.info("1. Ensure all dependencies are installed")
        logger.info("2. Check that the parent directory contains the required model files")
        logger.info("3. Verify CUDA/GPU setup if using GPU acceleration")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
