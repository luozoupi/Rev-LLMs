"""
Reversible Qwen3 with Candidate Selection Attention Integration - FIXED VERSION
===============================================================================

This module integrates reversible mechanisms from the Reformer with Qwen3 
candidate selection attention for maximum memory and computational efficiency.
"""
import math
try:
    from native_sparse_attention_pytorch import SparseAttention
    NATIVE_SPARSE_AVAILABLE = True
except Exception:
    NATIVE_SPARSE_AVAILABLE = False

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd.function import Function
from torch.utils.checkpoint import get_device_states, set_device_states
from typing import Optional, Tuple, List
from torch import Tensor
from native_sparse_attention_pytorch import SparseAttention
# Import from existing modules
import candi_sel_new_v2 as candi_sel_new
from transformers.models.qwen3.modeling_qwen3 import (
    Qwen3Config, 
    Qwen3RMSNorm, 
    repeat_kv, 
    apply_rotary_pos_emb,
    Qwen3MLP
)
from transformers.cache_utils import Cache

# NEW: safe rotary embedding import
try:
    from transformers.models.qwen3.modeling_qwen3 import Qwen3RotaryEmbedding
except Exception:
    Qwen3RotaryEmbedding = None
from transformers.cache_utils import Cache


class NativeSparseAttentionWrapper(nn.Module):
    """
    Wrapper for native sparse attention that handles integration with reversible blocks
    and proper memory management
    """
    def __init__(self, config, layer_idx: int, inner_dim: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.inner_dim = inner_dim
        
        if not NATIVE_SPARSE_AVAILABLE:
            raise ImportError("native_sparse_attention_pytorch is required for native_sparse attention_type")
        
        # Create the native sparse attention module
        # NOTE: SparseAttention handles its own QKV projections, normalization, and rotary embeddings
        self.sparse_attention = SparseAttention(
            dim=inner_dim,  # Use inner_dim instead of config.hidden_size for reversible compatibility
            dim_head=getattr(config, 'head_dim', inner_dim // config.num_attention_heads),
            heads=config.num_attention_heads,
            kv_heads=getattr(config, 'num_key_value_heads', config.num_attention_heads),
            causal=True,  # For language modeling
            sliding_window_size=getattr(config, 'sliding_window_size', 64),
            compress_block_size=getattr(config, 'compress_block_size', 16),
            compress_block_sliding_stride=getattr(config, 'compress_block_sliding_stride', 8),
            selection_block_size=getattr(config, 'selection_block_size', 16),
            num_selected_blocks=getattr(config, 'num_selected_blocks', 4),
            use_diff_topk=getattr(config, 'use_diff_topk', True),
            query_heads_share_selected_kv=getattr(config, 'query_heads_share_selected_kv', True),
            use_triton_kernel=getattr(config, 'use_triton_kernel', False),  # Disable Triton to reduce memory
            norm=False,  # We'll handle normalization externally for consistency
        )
        
        # Add external normalization for consistency with other attention types
        self.input_norm = Qwen3RMSNorm(inner_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
    
    def forward(
        self, 
        x, 
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs
    ):
        """
        Forward pass using native sparse attention with proper memory management
        """
        # Apply input normalization
        x = self.input_norm(x)
        
        # Native sparse attention handles rotary embeddings internally,
        # but we need to be careful about memory usage
        
        if past_key_values is not None:
            # For now, disable KV cache with native sparse attention to avoid memory issues
            print("Warning: Disabling KV cache for native sparse attention to reduce memory usage")
            past_key_values = None
        
        # Apply gradient checkpointing to reduce memory
        if self.training:
            return torch.utils.checkpoint.checkpoint(
                self._forward_sparse_attention,
                x,
                attention_mask,
                use_reentrant=False
            )
        else:
            return self._forward_sparse_attention(x, attention_mask)
    
    def _forward_sparse_attention(self, x, attention_mask):
        """Internal forward pass with memory optimization"""
        try:
            # Native sparse attention expects input of shape [batch, seq_len, dim]
            # It handles all QKV projections and attention computation internally
            attended_output = self.sparse_attention(x)
            return attended_output
        except torch.OutOfMemoryError as e:
            # Fallback: if OOM, reduce precision temporarily
            print(f"OOM in native sparse attention, trying half precision: {e}")
            with torch.cuda.amp.autocast():
                attended_output = self.sparse_attention(x.half()).float()
            return attended_output

class SimpleRotaryEmbedding(nn.Module):
    """Simple rotary embedding implementation for fallback"""
    
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        
        # Create frequency tensor - ensure it's on the right device
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
    
    def forward(self, x, position_ids=None):
        # Ensure all tensors are on the same device as input
        device = x.device
        
        if position_ids is None:
            seq_len = x.shape[1]
            position_ids = torch.arange(seq_len, device=device, dtype=torch.long)
            position_ids = position_ids.unsqueeze(0).expand(x.shape[0], -1)
        
        # Ensure position_ids is on the right device
        position_ids = position_ids.to(device)
        
        # Move inv_freq to the right device if needed
        if self.inv_freq.device != device:
            self.inv_freq = self.inv_freq.to(device)
        
        # Calculate sin and cos - ensure all on same device
        freqs = torch.einsum("i,j->ij", position_ids.flatten().float(), self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos().view(*position_ids.shape, -1)
        sin = emb.sin().view(*position_ids.shape, -1)
        
        return cos, sin


class Deterministic(nn.Module):
    """Deterministic wrapper for handling RNG state in reversible blocks"""
    def __init__(self, net):
        super().__init__()
        self.net = net
        self.cpu_state = None
        self.cuda_in_fwd = None
        self.gpu_devices = None
        self.gpu_states = None

    def record_rng(self, *args):
        self.cpu_state = torch.get_rng_state()
        if torch.cuda._initialized:
            self.cuda_in_fwd = True
            self.gpu_devices, self.gpu_states = get_device_states(*args)

    def forward(self, *args, record_rng=False, set_rng=False, **kwargs):
        if record_rng:
            self.record_rng(*args)

        if not set_rng:
            return self.net(*args, **kwargs)

        rng_devices = []
        if self.cuda_in_fwd:
            rng_devices = self.gpu_devices

        with torch.random.fork_rng(devices=rng_devices, enabled=True):
            torch.set_rng_state(self.cpu_state)
            if self.cuda_in_fwd:
                set_device_states(self.gpu_devices, self.gpu_states)
            return self.net(*args, **kwargs)
        
class ReversibleQwen3CandidateAttention(nn.Module):
    """Qwen3 Multi-type Attention adapted for reversible or standard networks"""
    def __init__(self, config, layer_idx: int, inner_dim: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        
        if inner_dim is None:
            inner_dim = config.hidden_size
        self.hidden_size = inner_dim
        self.num_attention_heads = config.num_attention_heads
        self.num_key_value_heads = getattr(config, 'num_key_value_heads', self.num_attention_heads)
        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
        self.head_dim = getattr(config, 'head_dim', self.hidden_size // self.num_attention_heads)
        self.attention_dropout = getattr(config, 'attention_dropout', 0.0)
        self.scaling = self.head_dim ** -0.5
        self.attention_type = getattr(config, 'attention_type', 'candidate_selection')

        # Initialize based on attention type
        if self.attention_type == "native_sparse":
            self.sparse_wrapper = NativeSparseAttentionWrapper(config, layer_idx, inner_dim)
        else:
            # Standard or candidate selection attention
            self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=False)
            self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
            self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
            self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=False)
            self.q_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
            self.k_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
            
            if self.attention_type == "candidate_selection":
                self.pr_ratio = getattr(config, 'candidate_pr_ratio', 0.5)
                self.top_k = getattr(config, 'candidate_top_k', 40)

        # Sliding window support
        self.sliding_window = None
        if hasattr(config, 'layer_types') and layer_idx < len(config.layer_types):
            if config.layer_types[layer_idx] == "sliding_attention":
                self.sliding_window = getattr(config, 'sliding_window', None) 
    
    def forward(
        self, 
        x, 
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs
    ):
        """Forward pass for multi-type attention"""
        if self.attention_type == "native_sparse":
            return self.sparse_wrapper(
                x, 
                position_embeddings=position_embeddings,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                cache_position=cache_position,
                **kwargs
            )
        else:
            return self._forward_manual_attention(
                x, 
                position_embeddings=position_embeddings,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                cache_position=cache_position
            )
    def _forward_manual_attention(
        self, 
        x, 
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ):
        """Stable manual attention with optional candidate selection pruning."""
        batch_size, seq_len, _ = x.shape
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        q = q.view(batch_size, seq_len, self.num_attention_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)

        q = self.q_norm(q)
        k = self.k_norm(k)

        q = q.transpose(1, 2)   # [B, Hq, L, D]
        k = k.transpose(1, 2)   # [B, Hk, L, D]
        v = v.transpose(1, 2)

        if position_embeddings is not None:
            cos, sin = position_embeddings
            q, k = apply_rotary_pos_emb(q, k, cos, sin)

        if past_key_values is not None:
            cache_kwargs = {}
            if position_embeddings is not None:
                cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            k, v = past_key_values.update(k, v, self.layer_idx, cache_kwargs)

        k = repeat_kv(k, self.num_key_value_groups)  # -> [B, Hq, L, D]
        v = repeat_kv(v, self.num_key_value_groups)

        attn_scores = torch.matmul(q, k.transpose(2, 3)) * self.scaling  # [B,H,L,L]

        # Apply provided causal / padding mask (already additive large negatives)
        if attention_mask is not None and attention_mask.dim() == 4:
            attn_scores = attn_scores + attention_mask[:, :, :, :k.shape[-2]]

        if self.attention_type == "candidate_selection":
            extended_attention_mask = None
            if attention_mask is not None:
                if attention_mask.dim() == 4:
                    extended_attention_mask = attention_mask
                elif attention_mask.dim() == 2:
                    extended_attention_mask = (1.0 - attention_mask.unsqueeze(1).unsqueeze(2)) * -1e9

            try:
                raw_mask = candi_sel_new.QK_prune_binary_quant(
                    q, k, extended_attention_mask,
                    pr_ratio=self.pr_ratio, top_k=self.top_k
                )
                # Normalize mask to boolean keep
                if raw_mask.dtype == torch.bool:
                    keep = raw_mask
                else:
                    # Assume values in [0,1]; keep > 0.5
                    keep = raw_mask > 0.5

                # Shape reconcile
                if keep.shape != attn_scores.shape:
                    # Accept [B,L,L] -> expand
                    if keep.dim() == 3 and keep.shape[1] == seq_len:
                        keep = keep.unsqueeze(1).expand(-1, attn_scores.size(1), -1, -1)
                    else:
                        # Fallback: keep everything
                        keep = torch.ones_like(attn_scores, dtype=torch.bool)

                # Force causal diagonal always kept
                diag = torch.eye(seq_len, device=attn_scores.device, dtype=torch.bool)
                keep = keep | diag.unsqueeze(0).unsqueeze(0)

                # Ensure at least one kept per row
                row_keep_count = keep.sum(dim=-1)  # [B,H,L]
                # Rows with zero (should not happen after diag) -> keep all
                if (row_keep_count == 0).any():
                    keep[row_keep_count == 0] = True

                # Large negative (finite) for pruned
                if attn_scores.dtype in (torch.float16, torch.bfloat16):
                    neg_fill = -1e4
                else:
                    neg_fill = -1e9
                attn_scores = attn_scores.masked_fill(~keep, neg_fill)

            except Exception as e:
                # Fallback: disable pruning this step
                # (Optionally log once)
                # print(f"[candidate_selection fallback] {e}")
                pass

        # Numerical stabilization
        attn_scores = attn_scores - attn_scores.max(dim=-1, keepdim=True).values
        attn_probs = F.softmax(attn_scores, dim=-1, dtype=torch.float32).to(q.dtype)
        attn_probs = F.dropout(attn_probs, p=self.attention_dropout, training=self.training)

        out = torch.matmul(attn_probs, v)             # [B,H,L,D]
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        out = self.o_proj(out)
        return out    
# class ReversibleQwen3CandidateAttention(nn.Module):
#     """Qwen3 Multi-type Attention adapted for reversible networks - FIXED VERSION"""
    
#     def __init__(self, config, layer_idx: int):
#         super().__init__()
#         self.config = config
#         self.layer_idx = layer_idx
#         self.hidden_size = config.hidden_size
        
#         # CRITICAL FIX: Proper handling of num_key_value_heads
#         # Ensure we never get 0 heads
#         self.num_attention_heads = max(1, config.num_attention_heads)
        
#         # Handle num_key_value_heads with proper fallback
#         if hasattr(config, 'num_key_value_heads') and config.num_key_value_heads is not None and config.num_key_value_heads > 0:
#             self.num_key_value_heads = max(1, config.num_key_value_heads)
#         else:
#             # Default to same as attention heads if not specified or invalid
#             self.num_key_value_heads = self.num_attention_heads
        
#         # Ensure valid head grouping
#         if self.num_attention_heads % self.num_key_value_heads != 0:
#             print(f"Warning: num_attention_heads ({self.num_attention_heads}) not divisible by num_key_value_heads ({self.num_key_value_heads})")
#             # Adjust num_key_value_heads to be a valid divisor
#             for divisor in [self.num_key_value_heads, self.num_key_value_heads // 2, 1]:
#                 if divisor > 0 and self.num_attention_heads % divisor == 0:
#                     self.num_key_value_heads = divisor
#                     print(f"Adjusted num_key_value_heads to {self.num_key_value_heads}")
#                     break
        
#         self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
#         self.head_dim = getattr(config, 'head_dim', config.hidden_size // config.num_attention_heads)
#         self.attention_dropout = getattr(config, 'attention_dropout', 0.0)
#         self.scaling = self.head_dim ** -0.5
        
#         # # Debug print to verify dimensions
#         # print(f"Layer {layer_idx} attention dimensions:")
#         # print(f"  num_attention_heads: {self.num_attention_heads}")
#         # print(f"  num_key_value_heads: {self.num_key_value_heads}")
#         # print(f"  head_dim: {self.head_dim}")
#         # print(f"  num_key_value_groups: {self.num_key_value_groups}")
        
#         # Attention type
#         self.attention_type = getattr(config, 'attention_type', 'candidate_selection')
        
#         # IMPORTANT: For reversible networks, after doubling and splitting,
#         # each half has the FULL hidden_size, not half of it
#         input_dim = self.hidden_size  # Not hidden_size // 2!
        
#         # Initialize based on attention type
#         if self.attention_type == "native_sparse":
#             # Try to use native sparse attention
#             try:
#                 self.sparse_attention = SparseAttention(
#                     dim=self.hidden_size,
#                     dim_head=self.head_dim,
#                     heads=self.num_attention_heads,
#                     kv_heads=self.num_key_value_heads,
#                     causal=True,
#                     sliding_window_size=getattr(config, 'sliding_window_size', 64),
#                     compress_block_size=getattr(config, 'compress_block_size', 16),
#                     compress_block_sliding_stride=getattr(config, 'compress_block_sliding_stride', 8),
#                     selection_block_size=getattr(config, 'selection_block_size', 16),
#                     num_selected_blocks=getattr(config, 'num_selected_blocks', 4),
#                     use_diff_topk=getattr(config, 'use_diff_topk', True),
#                     query_heads_share_selected_kv=getattr(config, 'query_heads_share_selected_kv', True),
#                     use_triton_kernel=getattr(config, 'use_triton_kernel', True),
#                 )
#             except Exception as e:
#                 print(f"Warning: Failed to initialize native sparse attention: {e}")
#                 print("Falling back to manual attention implementation")
#                 self.attention_type = "standard"
            
#         # For standard or candidate selection attention
#         if self.attention_type != "native_sparse":
#             # Verify projection dimensions before creating
#             q_proj_dim = self.num_attention_heads * self.head_dim
#             kv_proj_dim = self.num_key_value_heads * self.head_dim
            
#             # print(f"  Creating projections:")
#             # print(f"    input_dim: {input_dim}")
#             # print(f"    q_proj output: {q_proj_dim}")
#             # print(f"    k_proj output: {kv_proj_dim}")
#             # print(f"    v_proj output: {kv_proj_dim}")
            
#             # Verify dimensions are positive
#             assert q_proj_dim > 0, f"Invalid q_proj dimension: {q_proj_dim}"
#             assert kv_proj_dim > 0, f"Invalid kv_proj dimension: {kv_proj_dim}"
            
#             self.q_proj = nn.Linear(input_dim, q_proj_dim, bias=getattr(config, 'attention_bias', False))
#             self.k_proj = nn.Linear(input_dim, kv_proj_dim, bias=getattr(config, 'attention_bias', False))
#             self.v_proj = nn.Linear(input_dim, kv_proj_dim, bias=getattr(config, 'attention_bias', False))
#             self.o_proj = nn.Linear(q_proj_dim, input_dim, bias=getattr(config, 'attention_bias', False))
            
#             # Qwen3's RMSNorm for Q and K
#             self.q_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
#             self.k_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
        
#             # Candidate selection parameters
#             if self.attention_type == "candidate_selection":
#                 self.pr_ratio = getattr(config, 'candidate_pr_ratio', 0.5)
#                 self.top_k = getattr(config, 'candidate_top_k', 40)
        
#         # Sliding window support
#         self.sliding_window = None
#         if hasattr(config, 'layer_types') and layer_idx < len(config.layer_types):
#             if config.layer_types[layer_idx] == "sliding_attention":
#                 self.sliding_window = getattr(config, 'sliding_window', None)
    
#     def forward(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **kwargs
#     ):
#         """
#         Forward pass for multi-type attention - FIXED VERSION
#         """
#         # Add dimension validation
#         batch_size, seq_len, input_dim = x.shape
        
#         if self.attention_type == "native_sparse":
#             return self._forward_native_sparse(x, position_embeddings, attention_mask, past_key_values, cache_position)
#         else:
#             return self._forward_manual_attention(x, position_embeddings, attention_mask, past_key_values, cache_position)
    
#     def _forward_manual_attention(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#     ):
#         """
#         Forward pass using manual Q/K/V computation - FIXED VERSION
#         """
#         batch_size, seq_len, input_dim = x.shape
        
#         # Project to Q, K, V
#         query_proj = self.q_proj(x)
#         key_proj = self.k_proj(x)
#         value_proj = self.v_proj(x)
        
#         # Debug projection shapes
#         # print(f"Projection shapes - Q: {query_proj.shape}, K: {key_proj.shape}, V: {value_proj.shape}")
        
#         # Reshape to multi-head format with validation
#         expected_q_dim = self.num_attention_heads * self.head_dim
#         expected_kv_dim = self.num_key_value_heads * self.head_dim
        
#         assert query_proj.shape[-1] == expected_q_dim, f"Query projection mismatch: {query_proj.shape[-1]} vs {expected_q_dim}"
#         assert key_proj.shape[-1] == expected_kv_dim, f"Key projection mismatch: {key_proj.shape[-1]} vs {expected_kv_dim}"
#         assert value_proj.shape[-1] == expected_kv_dim, f"Value projection mismatch: {value_proj.shape[-1]} vs {expected_kv_dim}"
        
#         query_states = query_proj.view(batch_size, seq_len, self.num_attention_heads, self.head_dim)
#         key_states = key_proj.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
#         value_states = value_proj.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
        
#         # Apply RMS normalization
#         query_states = self.q_norm(query_states)
#         key_states = self.k_norm(key_states)
        
#         # Transpose to [batch, num_heads, seq_len, head_dim]
#         query_states = query_states.transpose(1, 2)
#         key_states = key_states.transpose(1, 2)
#         value_states = value_states.transpose(1, 2)
        
#         # Debug shapes before attention computation
#         # print(f"Pre-attention shapes - Q: {query_states.shape}, K: {key_states.shape}, V: {value_states.shape}")
        
#         # Apply rotary position embeddings if provided
#         if position_embeddings is not None:
#             cos, sin = position_embeddings
#             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        
#         # Handle past key values for KV cache
#         if past_key_values is not None:
#             cache_kwargs = {}
#             if position_embeddings is not None:
#                 cos, sin = position_embeddings
#                 cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
#             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
        
#         # Repeat KV heads for grouped query attention
#         key_states = repeat_kv(key_states, self.num_key_value_groups)
#         value_states = repeat_kv(value_states, self.num_key_value_groups)
        
#         # Final shape validation before attention computation
#         assert query_states.shape[1] == key_states.shape[1], f"Head count mismatch: Q={query_states.shape[1]}, K={key_states.shape[1]}"
        
#         # Compute attention scores
#         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling
        
#         # Apply causal mask
#         if attention_mask is not None:
#             if attention_mask.dim() == 4:
#                 causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]
#                 attn_weights = attn_weights + causal_mask
        
#         # Apply sparsity based on attention type
#         if self.attention_type == "candidate_selection":
#             # Prepare attention mask for candidate selection
#             extended_attention_mask = None
#             if attention_mask is not None:
#                 if attention_mask.dim() == 4:
#                     extended_attention_mask = attention_mask
#                 elif attention_mask.dim() == 2:
#                     extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
#                     extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
#                 else:
#                     extended_attention_mask = attention_mask
            
#             try:
#                 # Apply candidate selection pruning
#                 S_prune_mask = candi_sel_new.QK_prune_binary_quant(
#                     query_states, key_states, extended_attention_mask,
#                     pr_ratio=self.pr_ratio, top_k=self.top_k
#                 )
#                 attn_weights = attn_weights + S_prune_mask
#             except Exception as e:
#                 print(f"Warning: Candidate selection failed: {e}, using standard attention")
        
#         # Softmax and dropout
#         attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
#         attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        
#         # Compute output
#         attn_output = torch.matmul(attn_weights, value_states)
#         attn_output = attn_output.transpose(1, 2).contiguous()
#         attn_output = attn_output.reshape(batch_size, seq_len, -1)
#         attn_output = self.o_proj(attn_output)
        
#         return attn_output
    
#     def _forward_native_sparse(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#     ):
#         """Forward pass using Native Sparse Attention"""
#         if past_key_values is not None:
#             print("Warning: KV cache not yet supported with native sparse attention")
        
#         if position_embeddings is not None:
#             print("Warning: Rotary embeddings integration with native sparse attention needs custom implementation")
        
#         # Use the sparse attention module
#         attended_output = self.sparse_attention(x)
#         return attended_output
# class ReversibleQwen3CandidateAttention(nn.Module):
#     """Qwen3 Multi-type Attention adapted for reversible networks"""
    
#     def __init__(self, config, layer_idx: int):
#         super().__init__()
#         self.config = config
#         self.layer_idx = layer_idx
#         self.hidden_size = config.hidden_size
#         self.num_attention_heads = config.num_attention_heads
#         self.num_key_value_heads = getattr(config, 'num_key_value_heads', config.num_attention_heads)
#         self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
#         self.head_dim = getattr(config, 'head_dim', config.hidden_size // config.num_attention_heads)
#         self.attention_dropout = getattr(config, 'attention_dropout', 0.0)
#         self.scaling = self.head_dim ** -0.5
        
#         # Attention type
#         self.attention_type = getattr(config, 'attention_type', 'candidate_selection')
        
#         # IMPORTANT: For reversible networks, after doubling and splitting,
#         # each half has the FULL hidden_size, not half of it
#         input_dim = self.hidden_size  # Not hidden_size // 2!
        
#         # Initialize based on attention type
#         if self.attention_type == "native_sparse":

            
#             self.sparse_attention = SparseAttention(
#                 dim=self.hidden_size,
#                 dim_head=self.head_dim,
#                 heads=self.num_attention_heads,
#                 kv_heads=self.num_key_value_heads,
#                 causal=True,  # Assuming causal for language modeling
#                 sliding_window_size=getattr(config, 'sliding_window_size', 64),
#                 compress_block_size=getattr(config, 'compress_block_size', 16),
#                 compress_block_sliding_stride=getattr(config, 'compress_block_sliding_stride', 8),
#                 selection_block_size=getattr(config, 'selection_block_size', 16),
#                 num_selected_blocks=getattr(config, 'num_selected_blocks', 4),
#                 use_diff_topk=getattr(config, 'use_diff_topk', True),
#                 query_heads_share_selected_kv=getattr(config, 'query_heads_share_selected_kv', True),
#                 use_triton_kernel=getattr(config, 'use_triton_kernel', True),
#             )
            
#         else:
#             # Standard or Candidate Selection - need manual Q/K/V projections
#             self.q_proj = nn.Linear(input_dim, self.num_attention_heads * self.head_dim, 
#                                    bias=getattr(config, 'attention_bias', False))
#             self.k_proj = nn.Linear(input_dim, self.num_key_value_heads * self.head_dim, 
#                                    bias=getattr(config, 'attention_bias', False))
#             self.v_proj = nn.Linear(input_dim, self.num_key_value_heads * self.head_dim, 
#                                    bias=getattr(config, 'attention_bias', False))
#             self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, input_dim, 
#                                    bias=getattr(config, 'attention_bias', False))
            
#             # Qwen3's RMSNorm for Q and K
#             self.q_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
#             self.k_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
        
#             # Candidate selection parameters
#             if self.attention_type == "candidate_selection":
#                 self.pr_ratio = getattr(config, 'candidate_pr_ratio', 0.5)
#                 self.top_k = getattr(config, 'candidate_top_k', 40)
        
#         # Sliding window support
#         self.sliding_window = None
#         if hasattr(config, 'layer_types') and layer_idx < len(config.layer_types):
#             if config.layer_types[layer_idx] == "sliding_attention":
#                 self.sliding_window = getattr(config, 'sliding_window', None)
    
#     def forward(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **kwargs
#     ):
#         """
#         Forward pass for multi-type attention
#         """
#         if self.attention_type == "native_sparse":
#             return self._forward_native_sparse(x, position_embeddings, attention_mask, past_key_values, cache_position)
#         else:
#             return self._forward_manual_attention(x, position_embeddings, attention_mask, past_key_values, cache_position)
    
#     def _forward_native_sparse(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#     ):
#         """
#         Forward pass using Native Sparse Attention
#         """
#         # Native sparse attention handles everything internally
#         # It expects input tokens and returns attended tokens
        
#         # TODO: Handle position embeddings and KV cache for native sparse attention
#         # This might require modifications to the SparseAttention module or
#         # pre/post processing to integrate with position embeddings
        
#         if past_key_values is not None:
#             print("Warning: KV cache not yet supported with native sparse attention")
        
#         if position_embeddings is not None:
#             print("Warning: Rotary embeddings integration with native sparse attention needs custom implementation")
        
#         # For now, use the basic interface
#         # In production, you'd want to integrate position embeddings and caching
#         attended_output = self.sparse_attention(x)
        
#         return attended_output
    
#     def _forward_manual_attention(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#     ):
#         """
#         Forward pass using manual Q/K/V computation (candidate selection or standard)
#         """
#         batch_size, seq_len, input_dim = x.shape
        
#         # Project to Q, K, V
#         query_proj = self.q_proj(x)
#         key_proj = self.k_proj(x)
#         value_proj = self.v_proj(x)
        
#         # Reshape to multi-head format
#         query_states = query_proj.view(batch_size, seq_len, self.num_attention_heads, self.head_dim)
#         key_states = key_proj.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
#         value_states = value_proj.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
        
#         # Apply RMS normalization
#         query_states = self.q_norm(query_states)
#         key_states = self.k_norm(key_states)
        
#         # Transpose to [batch, num_heads, seq_len, head_dim]
#         query_states = query_states.transpose(1, 2)
#         key_states = key_states.transpose(1, 2)
#         value_states = value_states.transpose(1, 2)
        
#         # Apply rotary position embeddings if provided
#         if position_embeddings is not None:
#             cos, sin = position_embeddings
#             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        
#         # Handle past key values for KV cache
#         if past_key_values is not None:
#             cache_kwargs = {}
#             if position_embeddings is not None:
#                 cos, sin = position_embeddings
#                 cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
#             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
        
#         # Repeat KV heads for grouped query attention
#         key_states = repeat_kv(key_states, self.num_key_value_groups)
#         value_states = repeat_kv(value_states, self.num_key_value_groups)
        
#         # Compute attention scores
#         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling
        
#         # Apply causal mask
#         if attention_mask is not None:
#             if attention_mask.dim() == 4:
#                 causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]
#                 attn_weights = attn_weights + causal_mask
        
#         # Apply sparsity based on attention type
#         if self.attention_type == "candidate_selection":
#             # Prepare attention mask for candidate selection
#             extended_attention_mask = None
#             if attention_mask is not None:
#                 if attention_mask.dim() == 4:
#                     extended_attention_mask = attention_mask
#                 elif attention_mask.dim() == 2:
#                     extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
#                     extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
#                 else:
#                     extended_attention_mask = attention_mask
            
#             try:
#                 # Apply candidate selection pruning
#                 # print("Candidate selection attention applied.")
#                 S_prune_mask = candi_sel_new.QK_prune_binary_quant(
#                     query_states, key_states, extended_attention_mask,
#                     pr_ratio=self.pr_ratio, top_k=self.top_k
#                 )
#                 attn_weights = attn_weights + S_prune_mask
#             except Exception as e:
#                 print(f"Warning: Candidate selection failed: {e}, using standard attention")
        
#         # Softmax and dropout
#         attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
#         attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        
#         # Compute output
#         attn_output = torch.matmul(attn_weights, value_states)
#         attn_output = attn_output.transpose(1, 2).contiguous()
#         attn_output = attn_output.reshape(batch_size, seq_len, -1)
#         attn_output = self.o_proj(attn_output)
        
#         return attn_output

# class ReversibleQwen3CandidateAttention(nn.Module):
#     """Qwen3 Candidate Selection Attention adapted for reversible networks"""
    
#     def __init__(self, config, layer_idx: int):
#         super().__init__()
#         self.config = config
#         self.layer_idx = layer_idx
#         self.hidden_size = config.hidden_size
#         self.num_attention_heads = config.num_attention_heads
#         self.num_key_value_heads = getattr(config, 'num_key_value_heads', config.num_attention_heads)
#         self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads
#         self.head_dim = getattr(config, 'head_dim', config.hidden_size // config.num_attention_heads)
#         self.attention_dropout = getattr(config, 'attention_dropout', 0.0)
#         self.scaling = self.head_dim ** -0.5
        
#         # Candidate selection parameters
#         self.use_candidate_selection = getattr(config, 'use_candidate_selection', True)
#         self.pr_ratio = getattr(config, 'candidate_pr_ratio', 0.5)
#         self.top_k = getattr(config, 'candidate_top_k', 40)
        
#         # Sliding window support
#         self.sliding_window = None
#         if hasattr(config, 'layer_types') and layer_idx < len(config.layer_types):
#             if config.layer_types[layer_idx] == "sliding_attention":
#                 self.sliding_window = getattr(config, 'sliding_window', None)
        
#         # IMPORTANT: For reversible networks, after doubling and splitting,
#         # each half has the FULL hidden_size, not half of it
#         input_dim = self.hidden_size  # Not hidden_size // 2!
        
#         # Projections (adapted for actual split size)
#         self.q_proj = nn.Linear(input_dim, self.num_attention_heads * self.head_dim, 
#                                bias=getattr(config, 'attention_bias', False))
#         self.k_proj = nn.Linear(input_dim, self.num_key_value_heads * self.head_dim, 
#                                bias=getattr(config, 'attention_bias', False))
#         self.v_proj = nn.Linear(input_dim, self.num_key_value_heads * self.head_dim, 
#                                bias=getattr(config, 'attention_bias', False))
#         self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, input_dim, 
#                                bias=getattr(config, 'attention_bias', False))
        
#         # Qwen3's RMSNorm for Q and K
#         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
#         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=getattr(config, 'rms_norm_eps', 1e-6))
    
#     def forward(
#         self, 
#         x, 
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         past_key_values: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **kwargs
#     ):
#         """
#         Forward pass for reversible candidate attention
#         Args:
#             x: Input tensor - shape varies based on reversible vs standard mode
#         """
#         batch_size, seq_len, input_dim = x.shape
        
#         # Project to Q, K, V
#         query_proj = self.q_proj(x)
#         key_proj = self.k_proj(x)
#         value_proj = self.v_proj(x)
        
#         # Reshape to multi-head format
#         query_states = query_proj.view(batch_size, seq_len, self.num_attention_heads, self.head_dim)
#         key_states = key_proj.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
#         value_states = value_proj.view(batch_size, seq_len, self.num_key_value_heads, self.head_dim)
        
#         # Apply RMS normalization
#         query_states = self.q_norm(query_states)
#         key_states = self.k_norm(key_states)
        
#         # Transpose to [batch, num_heads, seq_len, head_dim]
#         query_states = query_states.transpose(1, 2)
#         key_states = key_states.transpose(1, 2)
#         value_states = value_states.transpose(1, 2)
        
#         # Apply rotary position embeddings if provided
#         if position_embeddings is not None:
#             cos, sin = position_embeddings
#             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        
#         # Handle past key values for KV cache
#         if past_key_values is not None:
#             cache_kwargs = {}
#             if position_embeddings is not None:
#                 cos, sin = position_embeddings
#                 cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
#             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)
        
#         # Repeat KV heads for grouped query attention
#         key_states = repeat_kv(key_states, self.num_key_value_groups)
#         value_states = repeat_kv(value_states, self.num_key_value_groups)
        
#         # Apply candidate selection attention
#         if self.config.attention_type == "candidate_selection":
#             # Prepare attention mask for candidate selection
#             extended_attention_mask = None
#             if attention_mask is not None:
#                 if attention_mask.dim() == 4:
#                     extended_attention_mask = attention_mask
#                 elif attention_mask.dim() == 2:
#                     extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
#                     extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
#                 else:
#                     extended_attention_mask = attention_mask
            
#             try:
#                 # Apply candidate selection pruning
#                 print("candidate selection attention applied.")
#                 S_prune_mask = candi_sel_new.QK_prune_binary_quant(
#                     query_states, key_states, extended_attention_mask,
#                     pr_ratio=self.pr_ratio, top_k=self.top_k
#                 )
#             except Exception as e:
#                 print(f"Warning: Candidate selection failed: {e}, using standard attention")
#                 S_prune_mask = torch.zeros_like(
#                     torch.matmul(query_states, key_states.transpose(2, 3))
#                 )

#         elif self.config.attention_type == "native_sparse":
#             self.attention = SparseAttention(
#                 dim=self.config.hidden_size,
#                 dim_head=self.config.head_dim,
#                 heads=self.config.num_attention_heads,
#                 kv_heads=self.config.num_key_value_heads,
#                 sliding_window_size=self.config.sliding_window_size,
#                 compress_block_size=self.config.compress_block_size,
#                 compress_block_sliding_stride=self.config.compress_block_sliding_stride,
#                 selection_block_size=self.config.selection_block_size,
#                 num_selected_blocks=self.config.num_selected_blocks,
#                 # ... other native sparse params
#             )
#         else:
#             S_prune_mask = torch.zeros_like(
#                 torch.matmul(query_states, key_states.transpose(2, 3))
#             )
#         # Compute attention scores
#         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling
        
#         # Apply causal mask
#         if attention_mask is not None:
#             if attention_mask.dim() == 4:
#                 causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]
#                 attn_weights = attn_weights + causal_mask
        
#         # Apply candidate selection pruning
#         attn_weights = attn_weights + S_prune_mask
        
#         # Softmax and dropout
#         attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
#         attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        
#         # Compute output
#         attn_output = torch.matmul(attn_weights, value_states)
#         attn_output = attn_output.transpose(1, 2).contiguous()
#         attn_output = attn_output.reshape(batch_size, seq_len, -1)
#         attn_output = self.o_proj(attn_output)
        
#         return attn_output


class ReversibleQwen3MLP(nn.Module):
    """Qwen3 MLP adapted for reversible networks"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        # After doubling and splitting, each half has the full hidden_size
        input_dim = config.hidden_size  # Not hidden_size // 2!
        
        # Adapt MLP for actual split size
        self.gate_proj = nn.Linear(input_dim, config.intermediate_size, bias=False)
        self.up_proj = nn.Linear(input_dim, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, input_dim, bias=False)
        self.act_fn = F.silu  # Qwen3 uses SiLU activation
    
    def forward(self, x):
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))


class ReversibleBlock(nn.Module):
    """Canonical reversible block (no grad graph saved in forward)"""
    def __init__(self, f, g, depth=None, send_signal=False):
        super().__init__()
        self.f = Deterministic(f)
        self.g = Deterministic(g)
        self.depth = depth
        self.send_signal = send_signal

    def forward(self, x, f_args={}, g_args={}):
        x1, x2 = torch.chunk(x, 2, dim=-1)
        if self.send_signal:
            f_args['_reverse'] = g_args['_reverse'] = False
            f_args['_depth']   = g_args['_depth']   = self.depth
        with torch.no_grad():
            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)
            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)
        return torch.cat([y1, y2], dim=-1)

    def backward_pass(self, y, dy, f_args={}, g_args={}):
        y1, y2  = torch.chunk(y, 2, dim=-1); del y
        dy1, dy2 = torch.chunk(dy, 2, dim=-1); del dy
        if self.send_signal:
            f_args['_reverse'] = g_args['_reverse'] = True
            f_args['_depth']   = g_args['_depth']   = self.depth
        with torch.enable_grad():
            y1.requires_grad = True
            gy1 = self.g(y1, set_rng=True, **g_args)
            torch.autograd.backward(gy1, dy2)
        with torch.no_grad():
            x2  = y2 - gy1; dx1 = dy1 + y1.grad
            y1.grad = None; del y2, gy1, dy1
        with torch.enable_grad():
            x2.requires_grad = True
            fx2 = self.f(x2, set_rng=True, **f_args)
            # torch.autograd.backward(fx2, dx1, retain_graph=True)
            torch.autograd.backward(fx2, dx1)
        with torch.no_grad():
            x1  = y1 - fx2; dx2 = dy2 + x2.grad
            x = torch.cat([x1, x2.detach()], dim=-1)
            dx = torch.cat([dx1, dx2], dim=-1)
            x2.grad = None; del y1, fx2, dy2
        return x, dx



class IrreversibleBlock(nn.Module):
    """Non-reversible block for short sequences (speed optimization)"""
    
    def __init__(self, f, g):
        super().__init__()
        self.f = f
        self.g = g

    def forward(self, x, f_args={}, g_args={}):
        x1, x2 = torch.chunk(x, 2, dim=2)
        y1 = x1 + self.f(x2, **f_args)
        y2 = x2 + self.g(y1, **g_args)
        return torch.cat([y1, y2], dim=2)


class _ReversibleFunction(Function):
    """Autograd function for reversible computation"""
    
    @staticmethod
    def forward(ctx, x, blocks, kwargs):
        ctx.kwargs = kwargs
        for block in blocks:
            x = block(x, **kwargs)
        ctx.y = x.detach()
        ctx.blocks = blocks
        return x

    @staticmethod
    def backward(ctx, dy):
        y = ctx.y
        kwargs = ctx.kwargs
        for block in ctx.blocks[::-1]:
            y, dy = block.backward_pass(y, dy, **kwargs)
        return dy, None, None


class ReversibleSequence(nn.Module):
    def __init__(self, blocks, layer_dropout=0., reverse_thres=0, send_signal=False):
        super().__init__()
        self.layer_dropout = layer_dropout
        self.reverse_thres = reverse_thres
        self.blocks = nn.ModuleList(
            [ReversibleBlock(f, g, depth=i, send_signal=send_signal) for i,(f,g) in enumerate(blocks)]
        )

    def forward(self, x, arg_route=(True, False), **kwargs):
        reverse = x.shape[1] > self.reverse_thres
        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)
        block_kwargs = {'f_args': f_args, 'g_args': g_args}

        if not reverse:
            # IMPORTANT: standard gradient path (no activation recompute)
            for blk in self.blocks:
                x1, x2 = torch.chunk(x, 2, dim=-1)
                y1 = x1 + blk.f.net(x2, **f_args)          # use underlying net, keep graph
                y2 = x2 + blk.g.net(y1, **g_args)
                x = torch.cat([y1, y2], dim=-1)
            return x

        # Reversible recompute path
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)



class Qwen3ReversibleCandidateConfig(Qwen3Config):
    """Extended Qwen3 config with candidate selection and reversible parameters"""
    
    def __init__(
        self,
        # use_candidate_selection=True,
        attention_type="candidate_selection",  # "candidate_selection", "native_sparse", "standard"
        candidate_pr_ratio=0.5,
        candidate_top_k=40,
        candidate_layers=None,
        candidate_use_checkpointing=False,  # Added missing attribute
        use_reversible=True,
        reverse_thres=1024,
        layer_dropout=0.0,
        sliding_window_size=64,
        compress_block_size=16,
        compress_block_sliding_stride=8,
        selection_block_size=16,
        num_selected_blocks=4,
        **kwargs
    ):
        super().__init__(**kwargs)
        # self.use_candidate_selection = use_candidate_selection
        self.candidate_pr_ratio = candidate_pr_ratio
        self.candidate_top_k = candidate_top_k
        self.candidate_layers = candidate_layers or list(range(self.num_hidden_layers))
        self.candidate_use_checkpointing = candidate_use_checkpointing  # Added missing attribute
        self.use_reversible = use_reversible
        self.reverse_thres = reverse_thres
        self.layer_dropout = layer_dropout
        self.attention_type = attention_type
        
        self.sliding_window_size = sliding_window_size
        self.compress_block_size = compress_block_size
        self.compress_block_sliding_stride = compress_block_sliding_stride
        self.selection_block_size = selection_block_size
        self.num_selected_blocks = num_selected_blocks

        # Ensure head dimension is set
        if not hasattr(self, 'head_dim'):
            self.head_dim = self.hidden_size // self.num_attention_heads
        
        # Set default rotary embedding parameters if not present
        if not hasattr(self, 'rope_theta'):
            self.rope_theta = 10000.0
        if not hasattr(self, 'max_position_embeddings'):
            self.max_position_embeddings = 2048

def get_reversible_training_config():
    """Optimized config for reversible training"""
    return {
        # Learning rate: Start higher for reversible models
        'learning_rate': 0.001,  # 10x higher than current 0.0001
        
        # Warmup schedule - critical for reversible models
        'warmup_steps': 1000,
        'warmup_ratio': 0.1,
        
        # Gradient settings
        'gradient_clip_norm': 0.5,  # Tighter clipping
        'gradient_accumulation_steps': 4,
        
        # Optimizer: AdamW with weight decay
        'optimizer': 'adamw',
        'weight_decay': 0.01,
        'betas': (0.9, 0.999),
        'eps': 1e-8,
        
        # Batch size: Smaller for better gradient estimates
        'batch_size': 16,
        'effective_batch_size': 64,  # Through accumulation
        
        # Training length
        'epochs': 50,  # More epochs needed
        'patience': 10,  # Early stopping patience
    }



class ReversibleQwen3DecoderLayer(nn.Module):
    """Single logical transformer layer (attention + MLP) with pre-norm f/g closures."""
    def __init__(self, config: Qwen3ReversibleCandidateConfig, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.attn = ReversibleQwen3CandidateAttention(config, layer_idx)
        self.mlp  = ReversibleQwen3MLP(config)
        self.pre_attn_ln = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.pre_mlp_ln  = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def f_func(self):
        attn = self.attn
        ln = self.pre_attn_ln
        def forward_f(x, **kw):
            return attn(ln(x), **kw)
        return forward_f

    def g_func(self):
        mlp = self.mlp
        ln  = self.pre_mlp_ln
        def forward_g(y, **kw):
            return mlp(ln(y))
        return forward_g


class StandardQwen3Wrapper(nn.Module):
    """Standard wrapper for non-reversible mode"""
    def __init__(self, config, layer_idx):
        super().__init__()
        self.attention = ReversibleQwen3CandidateAttention(config, layer_idx)
        self.mlp = ReversibleQwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
    
    def forward(self, hidden_states, position_embeddings=None, attention_mask=None, **kwargs):
        # Standard transformer forward pass
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states = self.attention(
            hidden_states, 
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            **kwargs
        )
        hidden_states = residual + hidden_states
        
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        
        return hidden_states


class ReversibleQwen3Model(nn.Module):
    def __init__(self, config: Qwen3ReversibleCandidateConfig):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)

        # Safe rotary embedding
        if Qwen3RotaryEmbedding is not None:
            try:
                self.rotary_emb = Qwen3RotaryEmbedding(config)
            except Exception:
                self.rotary_emb = SimpleRotaryEmbedding(
                    dim=config.head_dim if hasattr(config, 'head_dim') else (config.hidden_size // config.num_attention_heads),
                    max_position_embeddings=getattr(config, 'max_position_embeddings', 2048),
                    base=getattr(config, 'rope_theta', 10000.0),
                    device=self.embed_tokens.weight.device
                )
        else:
            self.rotary_emb = SimpleRotaryEmbedding(
                dim=config.head_dim if hasattr(config, 'head_dim') else (config.hidden_size // config.num_attention_heads),
                max_position_embeddings=getattr(config, 'max_position_embeddings', 2048),
                base=getattr(config, 'rope_theta', 10000.0),
                device=self.embed_tokens.weight.device
            )

        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # ALWAYS register layers so parameters move with .to(device)
        layers = [ReversibleQwen3DecoderLayer(config, i) for i in range(config.num_hidden_layers)]
        self.layers = nn.ModuleList(layers)

        if config.use_reversible:
            blocks = [(ly.f_func(), ly.g_func()) for ly in self.layers]
            self.rev_seq = ReversibleSequence(
                blocks,
                layer_dropout=config.layer_dropout,
                reverse_thres=config.reverse_thres,
                send_signal=True
            )
            self.is_reversible = True
        else:
            self.is_reversible = False

    def forward(self, input_ids, attention_mask=None, position_ids=None, **kw):
        x = self.embed_tokens(input_ids)
        if position_ids is None:
            position_ids = torch.arange(x.size(1), device=x.device).unsqueeze(0)
        cos, sin = self.rotary_emb(x, position_ids)

        if self.is_reversible:
            x_cat = torch.cat([x, x], dim=-1)
            f_args = dict(position_embeddings=(cos, sin), attention_mask=attention_mask)
            x_cat = self.rev_seq(x_cat, arg_route=(True, False), **f_args)
            x = torch.stack(x_cat.chunk(2, dim=-1), dim=0).mean(0)
        else:
            for layer in self.layers:
                x = layer.attn(layer.pre_attn_ln(x), position_embeddings=(cos, sin), attention_mask=attention_mask) + x
                x = layer.mlp(layer.pre_mlp_ln(x)) + x
        return self.norm(x)

class ReversibleQwen3ForCausalLM(nn.Module):
    """Qwen3 for causal language modeling with reversible layers and candidate selection"""
    
    def __init__(self, config: Qwen3ReversibleCandidateConfig):
        super().__init__()
        self.config = config
        self.model = ReversibleQwen3Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        
        # Tie weights if specified
        if getattr(config, 'tie_word_embeddings', False):
            self.lm_head.weight = self.model.embed_tokens.weight
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs
    ):
        # Forward through the model
        hidden_states = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs
        )
        
        # Get logits
        logits = self.lm_head(hidden_states)
        
        loss = None
        if labels is not None:
            # Compute loss
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, self.vocab_size), shift_labels.view(-1))
        
        return {
            'loss': loss,
            'logits': logits,
            'past_key_values': past_key_values,
            'hidden_states': hidden_states,
        }
    
    def generate_text(self, input_ids, max_length=100, temperature=1.0, do_sample=True):
        """Simple text generation method"""
        self.eval()
        generated = input_ids.clone()
        
        with torch.no_grad():
            for _ in range(max_length):
                outputs = self.forward(generated)
                logits = outputs['logits']
                next_token_logits = logits[:, -1, :] / temperature
                
                if do_sample:
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                generated = torch.cat([generated, next_token], dim=1)
                
                # Check for EOS token if defined
                if hasattr(self.config, 'eos_token_id') and next_token.item() == self.config.eos_token_id:
                    break
        
        return generated


def get_reversible_scheduler(optimizer, config):
    """Learning rate scheduler optimized for reversible training"""
    
    from torch.optim.lr_scheduler import OneCycleLR
    
    # Use OneCycle instead of ReduceLROnPlateau
    scheduler = OneCycleLR(
        optimizer,
        max_lr=config['learning_rate'],
        epochs=config['epochs'],
        steps_per_epoch=config['steps_per_epoch'],
        pct_start=0.3,  # 30% warmup
        anneal_strategy='cos',
        div_factor=25,  # Start with lr/25
        final_div_factor=1000,  # End with lr/1000
    )
    
    return scheduler

def train_reversible_model(model, train_loader, val_loader, config):
    """Fixed training loop for reversible models"""
    
    # Optimizer setup
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=config['betas'],
        eps=config['eps']
    )
    
    scheduler = get_reversible_scheduler(optimizer, config)
    criterion = nn.CrossEntropyLoss(ignore_index=-100)
    
    # Track best performance
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        model.train()
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            
            # Forward pass
            outputs = model(inputs)
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            
            # Compute loss
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = targets[..., 1:].contiguous()
            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), 
                           shift_labels.view(-1))
            
            # Scale loss for accumulation
            loss = loss / config['gradient_accumulation_steps']
            loss.backward()
            
            total_loss += loss.item()
            
            # Update weights
            if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:
                # Critical: Proper gradient clipping for reversible models
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(), 
                    config['gradient_clip_norm']
                )
                
                optimizer.step()
                scheduler.step()  # Step every update for OneCycle
                optimizer.zero_grad()
            
            num_batches += 1
        
        # Validation
        val_loss = validate_model(model, val_loader, criterion)
        
        print(f"Epoch {epoch+1}: Train Loss: {total_loss/num_batches:.4f}, "
              f"Val Loss: {val_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")
        
        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= config['patience']:
                print(f"Early stopping at epoch {epoch+1}")
                break

def create_reversible_qwen3_model(
    vocab_size=32000,
    hidden_size=4096,
    num_hidden_layers=32,
    num_attention_heads=32,
    num_key_value_heads=8,
    intermediate_size=11008,
    max_position_embeddings=2048,
    rope_theta=10000.0,
    rms_norm_eps=1e-6,
    # use_candidate_selection=True,
    attention_type="candidate_selection",
    candidate_pr_ratio=0.5,
    candidate_top_k=40,
    use_reversible=True,
    reverse_thres=1024,
    layer_dropout=0.0,
    **kwargs
):
    """Factory function to create a reversible Qwen3 model with candidate selection"""
    
    config = Qwen3ReversibleCandidateConfig(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        num_key_value_heads=num_key_value_heads,
        intermediate_size=intermediate_size,
        max_position_embeddings=max_position_embeddings,
        rope_theta=rope_theta,
        rms_norm_eps=rms_norm_eps,
        # use_candidate_selection=use_candidate_selection,
        attention_type=attention_type,
        candidate_pr_ratio=candidate_pr_ratio,
        candidate_top_k=candidate_top_k,
        use_reversible=use_reversible,
        reverse_thres=reverse_thres,
        layer_dropout=layer_dropout,
        **kwargs
    )
    
    return ReversibleQwen3ForCausalLM(config)

def create_fixed_reversible_qwen3_model(
    vocab_size=32000,
    hidden_size=4096,
    num_hidden_layers=32,
    num_attention_heads=32,
    num_key_value_heads=None,  # Will be set to valid default if None
    intermediate_size=11008,
    max_position_embeddings=2048,
    rope_theta=10000.0,
    rms_norm_eps=1e-6,
    attention_type="candidate_selection",
    candidate_pr_ratio=0.5,
    candidate_top_k=40,
    use_reversible=True,
    reverse_thres=1024,
    layer_dropout=0.0,
    **kwargs
):
    """Factory function to create a fixed reversible Qwen3 model"""
    
    # CRITICAL FIX: Ensure num_key_value_heads is properly set
    if num_key_value_heads is None or num_key_value_heads <= 0:
        num_key_value_heads = num_attention_heads  # Default to MHA
        print(f"Setting num_key_value_heads to {num_key_value_heads}")
    
    # Ensure it's a valid divisor
    if num_attention_heads % num_key_value_heads != 0:
        # Find the largest valid divisor
        for divisor in [num_key_value_heads, num_key_value_heads // 2, num_attention_heads // 4, num_attention_heads // 8, 1]:
            if divisor > 0 and num_attention_heads % divisor == 0:
                num_key_value_heads = divisor
                print(f"Adjusted num_key_value_heads to {num_key_value_heads} for proper grouping")
                break
    
    config = Qwen3ReversibleCandidateConfig(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        num_key_value_heads=num_key_value_heads,
        intermediate_size=intermediate_size,
        max_position_embeddings=max_position_embeddings,
        rope_theta=rope_theta,
        rms_norm_eps=rms_norm_eps,
        attention_type=attention_type,
        candidate_pr_ratio=candidate_pr_ratio,
        candidate_top_k=candidate_top_k,
        use_reversible=use_reversible,
        reverse_thres=reverse_thres,
        layer_dropout=layer_dropout,
        **kwargs
    )
    
    return ReversibleQwen3ForCausalLM(config)
# ==============================================================================
# TESTING AND VALIDATION
# ==============================================================================

def test_reversible_qwen3():
    """Test the reversible Qwen3 implementation"""
    print("Testing Reversible Qwen3 with Candidate Selection...")
    
    # Create a small test model
    model = create_reversible_qwen3_model(
        vocab_size=1000,
        hidden_size=512,
        num_hidden_layers=4,
        num_attention_heads=8,
        num_key_value_heads=4,
        intermediate_size=1024,
        # use_candidate_selection=True,
        attention_type="candidate_selection",
        candidate_pr_ratio=0.3,
        candidate_top_k=20,
        use_reversible=True,
        reverse_thres=512,  # Use reversible for sequences > 512
    )
    
    # Test with different sequence lengths
    batch_size = 2
    
    for seq_len in [256, 512, 1024]:  # Short, threshold, long
        print(f"\n--- Testing sequence length: {seq_len} ---")
        
        # Create test input
        input_ids = torch.randint(0, 1000, (batch_size, seq_len))
        
        try:
            # Forward pass
            outputs = model(input_ids)
            logits = outputs['logits']
            
            print(f" Forward pass successful!")
            print(f"   Input shape: {input_ids.shape}")
            print(f"   Output shape: {logits.shape}")
            print(f"   Using reversible: {seq_len > model.config.reverse_thres}")
            
            # Test backward pass
            loss = outputs['logits'].sum()
            loss.backward()
            print(f" Backward pass successful!")
            
            # Test memory efficiency by comparing gradients
            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            params_with_grad = sum(p.numel() for p in model.parameters() if p.grad is not None)
            print(f"   Parameters with gradients: {params_with_grad}/{total_params}")
            
        except Exception as e:
            print(f" Test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    print(f"\n All tests passed! Reversible Qwen3 with candidate selection is working.")
    return True

def create_fixed_reversible_qwen3_model(**kwargs):
    """Create Qwen3 model with fixed reversible implementation"""
    
    # Get the original model first
    model = create_reversible_qwen3_model(**kwargs)
    
    # # Replace reversible blocks with fixed ones
    # if hasattr(model.model, 'decoder_layers'):
    #     for layer in model.model.decoder_layers:
    #         if hasattr(layer, 'reversible_sequence'):
    #             # Update the reversible blocks
    #             for block in layer.reversible_sequence.blocks:
    #                 block.__class__ = ImprovedReversibleBlock
    #                 block.grad_scale = nn.Parameter(torch.ones(1))
    
    # Apply proper initialization
    # model = init_reversible_model(model)
    
    return model


def train_with_mixed_precision(model, train_loader, val_loader, config):
    """Mixed precision training for reversible models"""
    
    from torch.cuda.amp import GradScaler, autocast
    
    scaler = GradScaler()
    optimizer = torch.optim.AdamW(model.parameters(), **config['optimizer_kwargs'])
    
    for epoch in range(config['epochs']):
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            with autocast():  # Enable mixed precision
                outputs = model(inputs)
                loss = compute_loss(outputs, targets)
                loss = loss / config['gradient_accumulation_steps']
            
            # Scale loss and backward
            scaler.scale(loss).backward()
            
            if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:
                scaler.unscale_(optimizer)  # Unscale for clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
def compare_memory_usage():
    """Compare memory usage between standard and reversible models (fixed)."""
    print("\n=== Memory Usage Comparison ===")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch_size = 4
    seq_len = 1024
    vocab_size = 16000
    hidden_size = 2048
    n_layers = 6
    heads = 16
    kv_heads = 8
    inter = hidden_size * 4

    configs = [
        ("Standard Qwen3", "standard", False),
        ("Candidate Selection", "candidate_selection", False),
        ("Reversible", "standard", True),
        ("Reversible + Candidate", "candidate_selection", True),
        ("Reversible + native_sparse", "native_sparse", True),
    ]

    if device.type == 'cuda':
        torch.cuda.empty_cache()

    for name, attention_type, use_reversible in configs:
        print(f"\n{name}:")
        model = None
        input_ids = None
        outputs = None
        loss = None
        try:
            if device.type == 'cuda':
                torch.cuda.reset_peak_memory_stats()

            model = create_reversible_qwen3_model(
                vocab_size=vocab_size,
                hidden_size=hidden_size,
                num_hidden_layers=n_layers,
                num_attention_heads=heads,
                num_key_value_heads=kv_heads,
                intermediate_size=inter,
                attention_type=attention_type,
                use_reversible=use_reversible,
                reverse_thres=0 if use_reversible else 10**9
            ).to(device)

            model.train()  # ensure grads

            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"  Total parameters: {total_params:,}")
            print(f"  Trainable parameters: {trainable_params:,}")
            print(f"  Parameter size (float32): {total_params*4/1024**2:.2f} MB")

            input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)

            autocast_enabled = (device.type == 'cuda')
            # Modern autocast API (PyTorch >= 2.0)
            ctx = torch.autocast("cuda") if autocast_enabled else torch.enable_grad()
            with ctx:
                outputs = model(input_ids)
                logits = outputs['logits']
                # Use only a small slice for backward to reduce memory
                slice_logits = logits[:, -128:, :]
                loss = slice_logits.mean()

            loss.backward()

            if device.type == 'cuda':
                peak = torch.cuda.max_memory_allocated() / 1024**2
                print(f"  Peak GPU memory: {peak:.2f} MB")

        except torch.cuda.OutOfMemoryError as oom:
            print(f"  OOM: {oom}")
        except Exception as e:
            print(f"  Failed: {e}")
            import traceback; traceback.print_exc()
        finally:
            for obj_name in ["model", "input_ids", "outputs", "loss"]:
                if obj_name in locals():
                    del locals()[obj_name]
            if device.type == 'cuda':
                torch.cuda.empty_cache()



# if __name__ == "__main__":
#     # Run tests
#     print("="*60)
#     print("REVERSIBLE QWEN3 WITH CANDIDATE SELECTION")
#     print("="*60)
    
#     success = test_reversible_qwen3()
    
#     if success:
#         print("\n" + "="*60)
#         print("MEMORY EFFICIENCY COMPARISON")
#         print("="*60)
#         compare_memory_usage()
        
#         print("\n" + "="*60)
#         print("INTEGRATION COMPLETE!")
#         print("="*60)
#         print(" Reversible mechanisms successfully integrated")
#         print(" Candidate selection attention preserved") 
#         print(" Memory efficiency achieved")
#         print(" Backward compatibility maintained")
        
#         print("\nNext steps:")
#         print("1. Fine-tune hyperparameters (pr_ratio, top_k, reverse_thres)")
#         print("2. Test on your specific datasets")
#         print("3. Compare with baseline models")
#         print("4. Scale up to full model size")



def create_reversible_qwen3_model(
    vocab_size=32000,
    hidden_size=4096,
    num_hidden_layers=32,
    num_attention_heads=32,
    num_key_value_heads=8,
    intermediate_size=11008,
    max_position_embeddings=2048,
    rope_theta=10000.0,
    rms_norm_eps=1e-6,
    # use_candidate_selection=True,
    attention_type="candidate_selection",
    candidate_pr_ratio=0.5,
    candidate_top_k=40,
    use_reversible=True,
    reverse_thres=1024,
    layer_dropout=0.0,
    **kwargs
):
    """Factory function to create a reversible Qwen3 model with candidate selection"""
    
    config = Qwen3ReversibleCandidateConfig(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        num_key_value_heads=num_key_value_heads,
        intermediate_size=intermediate_size,
        max_position_embeddings=max_position_embeddings,
        rope_theta=rope_theta,
        rms_norm_eps=rms_norm_eps,
        # use_candidate_selection=use_candidate_selection,
        attention_type=attention_type,
        candidate_pr_ratio=candidate_pr_ratio,
        candidate_top_k=candidate_top_k,
        use_reversible=use_reversible,
        reverse_thres=reverse_thres,
        layer_dropout=layer_dropout,
        **kwargs
    )
    
    return ReversibleQwen3ForCausalLM(config)


# ==============================================================================
# TESTING AND VALIDATION
# ==============================================================================

def test_reversible_qwen3():
    """Test the reversible Qwen3 implementation"""
    print("Testing Reversible Qwen3 with Candidate Selection...")
    
    # Create a small test model
    model = create_reversible_qwen3_model(
        vocab_size=1000,
        hidden_size=512,
        num_hidden_layers=6,
        num_attention_heads=8,
        num_key_value_heads=4,
        intermediate_size=1024,
        # use_candidate_selection=True,
        attention_type="candidate_selection",
        candidate_pr_ratio=0.7,
        candidate_top_k=20,
        use_reversible=True,
        reverse_thres=512,  # Use reversible for sequences > 512
    )
    
    # Test with different sequence lengths
    batch_size = 2
    
    for seq_len in [256, 512, 1024]:  # Short, threshold, long
        print(f"\n--- Testing sequence length: {seq_len} ---")
        
        # Create test input
        input_ids = torch.randint(0, 1000, (batch_size, seq_len))
        
        try:
            # Forward pass
            outputs = model(input_ids)
            logits = outputs['logits']
            
            print(f" Forward pass successful!")
            print(f"   Input shape: {input_ids.shape}")
            print(f"   Output shape: {logits.shape}")
            print(f"   Using reversible: {seq_len > model.config.reverse_thres}")
            
            # Test backward pass
            loss = outputs['logits'].sum()
            loss.backward()
            print(f" Backward pass successful!")
            
            # Test memory efficiency by comparing gradients
            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            params_with_grad = sum(p.numel() for p in model.parameters() if p.grad is not None)
            print(f"   Parameters with gradients: {params_with_grad}/{total_params}")
            
        except Exception as e:
            print(f" Test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    print(f"\n All tests passed! Reversible Qwen3 with candidate selection is working.")
    return True


# def compare_memory_usage():
#     """Compare memory usage with improved memory management"""
#     print("\n=== Memory Usage Comparison (Optimized) ===")
    
#     # Reduced test parameters to avoid OOM
#     batch_size = 1*10  # Reduced from 1*5
#     seq_len = 1024*2  # Reduced from 2048
#     vocab_size = 1000*10  # Reduced from 1000*10*2
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
#     configs = [
#         ("Standard Qwen3", "standard", False),
#         ("Candidate Selection", "candidate_selection", False), 
#         ("Reversible", "standard", True),
#         ("Reversible + Candidate", "candidate_selection", True),
#         ("Reversible + native_sparse", "native_sparse", True)
#     ]
    
#     for name, attention_type, use_reversible in configs:
#         print(f"\n{name}:")
        
#         try:
#             model = create_reversible_qwen3_model(
#                 vocab_size=vocab_size,
#                 hidden_size=256,  # Reduced from 512*4
#                 num_hidden_layers=2,  # Reduced from 6
#                 num_attention_heads=8,
#                 num_key_value_heads=4,
#                 intermediate_size=512,  # Reduced proportionally
#                 attention_type=attention_type,
#                 use_reversible=use_reversible,
#                 reverse_thres=512 if use_reversible else 999999,  # Lower threshold
#                 sliding_window_size=32,  # Reduced from 64
#                 compress_block_size=8,   # Reduced from 16
#                 compress_block_sliding_stride=4,  # Reduced from 8
#                 selection_block_size=8,  # Reduced from 16
#                 num_selected_blocks=2    # Reduced from 4
#             )
#             model = model.to(device)
            
#             # Clear cache and measure memory
#             if torch.cuda.is_available():
#                 torch.cuda.empty_cache()
#                 torch.cuda.reset_peak_memory_stats()
            
#             # Create input AFTER model is on device
#             input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
            
#             # Forward and backward pass with memory optimization
#             with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):
#                 outputs = model(input_ids)
#                 loss = outputs['logits'].sum() * 0.001  # Scale down loss to reduce gradients
            
#             # Use gradient accumulation to reduce memory
#             loss.backward()
            
#             if torch.cuda.is_available():
#                 peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
#                 print(f"  Peak GPU memory: {peak_memory:.2f} MB")
#             else:
#                 print(f"  CPU-only test completed")
            
#             # Clean up
#             del model, input_ids, outputs, loss
#             if torch.cuda.is_available():
#                 torch.cuda.empty_cache()
                
#         except Exception as e:
#             print(f"  Failed: {e}")
#             # Clean up on error
#             if torch.cuda.is_available():
#                 torch.cuda.empty_cache()


if __name__ == "__main__":
    # Run tests
    print("="*60)
    print("REVERSIBLE QWEN3 WITH CANDIDATE SELECTION")
    print("="*60)
    
    success = test_reversible_qwen3()
    
    if success:
        print("\n" + "="*60)
        print("MEMORY EFFICIENCY COMPARISON")
        print("="*60)
        compare_memory_usage()
        
        print("\n" + "="*60)
        print("INTEGRATION COMPLETE!")
        print("="*60)
        print(" Reversible mechanisms successfully integrated")
        print(" Candidate selection attention preserved") 
        print(" Memory efficiency achieved")
        print(" Backward compatibility maintained")
        
        print("\nNext steps:")
        print("1. Fine-tune hyperparameters (pr_ratio, top_k, reverse_thres)")
        print("2. Test on your specific datasets")
        print("3. Compare with baseline models")
        print("4. Scale up to full model size")